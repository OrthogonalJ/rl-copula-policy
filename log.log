2019-12-10 20:52:17,286	WARNING worker.py:672 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.
2019-12-10 20:52:17,322	INFO resource_spec.py:216 -- Starting Ray with 158.4 GiB memory available for workers and up to 18.63 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2019-12-10 20:52:17,582	WARNING services.py:1009 -- Failed to start the reporter. The reporter requires 'pip install psutil'.
WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.
== Status ==
Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)
Using FIFO scheduling algorithm.
Resources requested: 8/48 CPUs, 0/3 GPUs, 0.0/158.4 GiB heap, 0.0/12.84 GiB objects
Number of trials: 1 ({'RUNNING': 1})
Result logdir: /media/tb-2/user_data/jordan/projects/rl-copula-policy/data/gaussian_copula_v3_rnn_model_ReversedAddition3-v0-20191210T205217/gaussian_copula_v3_rnn_model_ReversedAddition3-v0
+--------------------------------------------------+----------+-------+
| Trial name                                       | status   | loc   |
|--------------------------------------------------+----------+-------|
| PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a | RUNNING  |       |
+--------------------------------------------------+----------+-------+

[2m[36m(pid=406245)[0m 2019-12-10 20:52:20,114	INFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=406245)[0m 2019-12-10 20:52:20,115	INFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=406245)[0m num_outputs: 15
[2m[36m(pid=406245)[0m cell_size: 64
[2m[36m(pid=406245)[0m Model: "model"
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=406245)[0m ==================================================================================================
[2m[36m(pid=406245)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m obs (InputLayer)                [(None, None, 4)]    0                                            
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m dense (Dense)                   (None, None, 64)     320         obs[0][0]                        
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m h_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m c_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406245)[0m                                                                  tf_op_layer_default_policy/Sequen
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m lstm (LSTM)                     [(None, None, 64), ( 33024       dense[0][0]                      
[2m[36m(pid=406245)[0m                                                                  h_in[0][0]                       
[2m[36m(pid=406245)[0m                                                                  c_in[0][0]                       
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m policy_flat_action_prams (Dense (None, None, 15)     975         lstm[0][0]                       
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m vf_out (Dense)                  (None, None, 1)      65          lstm[0][0]                       
[2m[36m(pid=406245)[0m ==================================================================================================
[2m[36m(pid=406245)[0m Total params: 34,384
[2m[36m(pid=406245)[0m Trainable params: 34,384
[2m[36m(pid=406245)[0m Non-trainable params: 0
[2m[36m(pid=406245)[0m __________________________________________________________________________________________________
[2m[36m(pid=406245)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406245)[0m inputs: Tensor("default_policy/Reshape_1:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406245)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406245)[0m COMPUTING PG POLICY LOSS
[2m[36m(pid=406245)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406245)[0m inputs: Tensor("default_policy/Reshape_10:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406245)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406245)[0m inputs: Tensor("default_policy/Reshape_13:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406245)[0m 2019-12-10 20:52:23,221	WARNING util.py:48 -- Install psutil to monitor system performance.
[2m[36m(pid=406230)[0m num_outputs: 15
[2m[36m(pid=406202)[0m num_outputs: 15
[2m[36m(pid=406233)[0m num_outputs: 15
[2m[36m(pid=406233)[0m cell_size: 64
[2m[36m(pid=406230)[0m cell_size: 64
[2m[36m(pid=406202)[0m cell_size: 64
[2m[36m(pid=406216)[0m num_outputs: 15
[2m[36m(pid=406216)[0m cell_size: 64
[2m[36m(pid=406195)[0m num_outputs: 15
[2m[36m(pid=406228)[0m num_outputs: 15
[2m[36m(pid=406195)[0m cell_size: 64
[2m[36m(pid=406228)[0m cell_size: 64
[2m[36m(pid=406240)[0m num_outputs: 15
[2m[36m(pid=406240)[0m cell_size: 64
[2m[36m(pid=406233)[0m Model: "model"
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=406233)[0m ==================================================================================================
[2m[36m(pid=406233)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m obs (InputLayer)                [(None, None, 4)]    0                                            
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m dense (Dense)                   (None, None, 64)     320         obs[0][0]                        
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m h_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m c_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406233)[0m                                                                  tf_op_layer_default_policy/Sequen
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m lstm (LSTM)                     [(None, None, 64), ( 33024       dense[0][0]                      
[2m[36m(pid=406233)[0m                                                                  h_in[0][0]                       
[2m[36m(pid=406233)[0m                                                                  c_in[0][0]                       
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m policy_flat_action_prams (Dense (None, None, 15)     975         lstm[0][0]                       
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m vf_out (Dense)                  (None, None, 1)      65          lstm[0][0]                       
[2m[36m(pid=406233)[0m ==================================================================================================
[2m[36m(pid=406233)[0m Total params: 34,384
[2m[36m(pid=406233)[0m Trainable params: 34,384
[2m[36m(pid=406233)[0m Non-trainable params: 0
[2m[36m(pid=406233)[0m __________________________________________________________________________________________________
[2m[36m(pid=406233)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406230)[0m Model: "model"
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=406230)[0m ==================================================================================================
[2m[36m(pid=406230)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m obs (InputLayer)                [(None, None, 4)]    0                                            
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m dense (Dense)                   (None, None, 64)     320         obs[0][0]                        
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m h_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m c_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406230)[0m                                                                  tf_op_layer_default_policy/Sequen
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m lstm (LSTM)                     [(None, None, 64), ( 33024       dense[0][0]                      
[2m[36m(pid=406230)[0m                                                                  h_in[0][0]                       
[2m[36m(pid=406230)[0m                                                                  c_in[0][0]                       
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m policy_flat_action_prams (Dense (None, None, 15)     975         lstm[0][0]                       
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m vf_out (Dense)                  (None, None, 1)      65          lstm[0][0]                       
[2m[36m(pid=406230)[0m ==================================================================================================
[2m[36m(pid=406230)[0m Total params: 34,384
[2m[36m(pid=406230)[0m Trainable params: 34,384
[2m[36m(pid=406230)[0m Non-trainable params: 0
[2m[36m(pid=406230)[0m __________________________________________________________________________________________________
[2m[36m(pid=406230)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406202)[0m Model: "model"
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=406202)[0m ==================================================================================================
[2m[36m(pid=406202)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m obs (InputLayer)                [(None, None, 4)]    0                                            
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m dense (Dense)                   (None, None, 64)     320         obs[0][0]                        
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m h_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m c_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406202)[0m                                                                  tf_op_layer_default_policy/Sequen
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m lstm (LSTM)                     [(None, None, 64), ( 33024       dense[0][0]                      
[2m[36m(pid=406202)[0m                                                                  h_in[0][0]                       
[2m[36m(pid=406202)[0m                                                                  c_in[0][0]                       
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m policy_flat_action_prams (Dense (None, None, 15)     975         lstm[0][0]                       
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m vf_out (Dense)                  (None, None, 1)      65          lstm[0][0]                       
[2m[36m(pid=406202)[0m ==================================================================================================
[2m[36m(pid=406202)[0m Total params: 34,384
[2m[36m(pid=406202)[0m Trainable params: 34,384
[2m[36m(pid=406202)[0m Non-trainable params: 0
[2m[36m(pid=406202)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406216)[0m Model: "model"
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=406216)[0m ==================================================================================================
[2m[36m(pid=406216)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m obs (InputLayer)                [(None, None, 4)]    0                                            
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m dense (Dense)                   (None, None, 64)     320         obs[0][0]                        
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m h_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m c_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406216)[0m                                                                  tf_op_layer_default_policy/Sequen
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m lstm (LSTM)                     [(None, None, 64), ( 33024       dense[0][0]                      
[2m[36m(pid=406216)[0m                                                                  h_in[0][0]                       
[2m[36m(pid=406216)[0m                                                                  c_in[0][0]                       
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m policy_flat_action_prams (Dense (None, None, 15)     975         lstm[0][0]                       
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m vf_out (Dense)                  (None, None, 1)      65          lstm[0][0]                       
[2m[36m(pid=406216)[0m ==================================================================================================
[2m[36m(pid=406216)[0m Total params: 34,384
[2m[36m(pid=406216)[0m Trainable params: 34,384
[2m[36m(pid=406216)[0m Non-trainable params: 0
[2m[36m(pid=406216)[0m __________________________________________________________________________________________________
[2m[36m(pid=406216)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406195)[0m Model: "model"
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=406195)[0m ==================================================================================================
[2m[36m(pid=406195)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m obs (InputLayer)                [(None, None, 4)]    0                                            
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m dense (Dense)                   (None, None, 64)     320         obs[0][0]                        
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m h_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m c_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406195)[0m                                                                  tf_op_layer_default_policy/Sequen
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m lstm (LSTM)                     [(None, None, 64), ( 33024       dense[0][0]                      
[2m[36m(pid=406195)[0m                                                                  h_in[0][0]                       
[2m[36m(pid=406195)[0m                                                                  c_in[0][0]                       
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m policy_flat_action_prams (Dense (None, None, 15)     975         lstm[0][0]                       
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m vf_out (Dense)                  (None, None, 1)      65          lstm[0][0]                       
[2m[36m(pid=406195)[0m ==================================================================================================
[2m[36m(pid=406195)[0m Total params: 34,384
[2m[36m(pid=406195)[0m Trainable params: 34,384
[2m[36m(pid=406195)[0m Non-trainable params: 0
[2m[36m(pid=406195)[0m __________________________________________________________________________________________________
[2m[36m(pid=406195)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406228)[0m Model: "model"
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=406228)[0m ==================================================================================================
[2m[36m(pid=406228)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m obs (InputLayer)                [(None, None, 4)]    0                                            
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m dense (Dense)                   (None, None, 64)     320         obs[0][0]                        
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m h_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m c_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406228)[0m                                                                  tf_op_layer_default_policy/Sequen
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m lstm (LSTM)                     [(None, None, 64), ( 33024       dense[0][0]                      
[2m[36m(pid=406228)[0m                                                                  h_in[0][0]                       
[2m[36m(pid=406228)[0m                                                                  c_in[0][0]                       
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m policy_flat_action_prams (Dense (None, None, 15)     975         lstm[0][0]                       
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m vf_out (Dense)                  (None, None, 1)      65          lstm[0][0]                       
[2m[36m(pid=406228)[0m ==================================================================================================
[2m[36m(pid=406228)[0m Total params: 34,384
[2m[36m(pid=406228)[0m Trainable params: 34,384
[2m[36m(pid=406228)[0m Non-trainable params: 0
[2m[36m(pid=406228)[0m __________________________________________________________________________________________________
[2m[36m(pid=406228)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406230)[0m inputs: Tensor("default_policy/Reshape_1:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406240)[0m Model: "model"
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=406240)[0m ==================================================================================================
[2m[36m(pid=406240)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m obs (InputLayer)                [(None, None, 4)]    0                                            
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m dense (Dense)                   (None, None, 64)     320         obs[0][0]                        
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m h_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m c_in (InputLayer)               [(None, 64)]         0                                            
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen
[2m[36m(pid=406240)[0m                                                                  tf_op_layer_default_policy/Sequen
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m lstm (LSTM)                     [(None, None, 64), ( 33024       dense[0][0]                      
[2m[36m(pid=406240)[0m                                                                  h_in[0][0]                       
[2m[36m(pid=406240)[0m                                                                  c_in[0][0]                       
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m policy_flat_action_prams (Dense (None, None, 15)     975         lstm[0][0]                       
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406240)[0m vf_out (Dense)                  (None, None, 1)      65          lstm[0][0]                       
[2m[36m(pid=406240)[0m ==================================================================================================
[2m[36m(pid=406240)[0m Total params: 34,384
[2m[36m(pid=406240)[0m Trainable params: 34,384
[2m[36m(pid=406240)[0m Non-trainable params: 0
[2m[36m(pid=406240)[0m __________________________________________________________________________________________________
[2m[36m(pid=406202)[0m inputs: Tensor("default_policy/Reshape_1:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406233)[0m inputs: Tensor("default_policy/Reshape_1:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406240)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406216)[0m inputs: Tensor("default_policy/Reshape_1:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406195)[0m inputs: Tensor("default_policy/Reshape_1:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406228)[0m inputs: Tensor("default_policy/Reshape_1:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406240)[0m inputs: Tensor("default_policy/Reshape_1:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406230)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406202)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406233)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406216)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406230)[0m COMPUTING PG POLICY LOSS
[2m[36m(pid=406230)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406202)[0m COMPUTING PG POLICY LOSS
[2m[36m(pid=406233)[0m COMPUTING PG POLICY LOSS
[2m[36m(pid=406233)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406195)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406228)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406240)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406202)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406216)[0m COMPUTING PG POLICY LOSS
[2m[36m(pid=406216)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406233)[0m inputs: Tensor("default_policy/Reshape_10:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406230)[0m inputs: Tensor("default_policy/Reshape_10:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406228)[0m COMPUTING PG POLICY LOSS
[2m[36m(pid=406228)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406240)[0m COMPUTING PG POLICY LOSS
[2m[36m(pid=406240)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406202)[0m inputs: Tensor("default_policy/Reshape_10:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406216)[0m inputs: Tensor("default_policy/Reshape_10:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406195)[0m COMPUTING PG POLICY LOSS
[2m[36m(pid=406195)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406228)[0m inputs: Tensor("default_policy/Reshape_10:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406230)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406233)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406202)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406195)[0m inputs: Tensor("default_policy/Reshape_10:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406240)[0m inputs: Tensor("default_policy/Reshape_10:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406216)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406230)[0m inputs: Tensor("default_policy/Reshape_13:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406233)[0m inputs: Tensor("default_policy/Reshape_13:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406228)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406240)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406202)[0m inputs: Tensor("default_policy/Reshape_13:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406216)[0m inputs: Tensor("default_policy/Reshape_13:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406195)[0m forward_rnn$seq_lens: Tensor("default_policy/seq_lens:0", shape=(?,), dtype=int32)
[2m[36m(pid=406228)[0m inputs: Tensor("default_policy/Reshape_13:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406240)[0m inputs: Tensor("default_policy/Reshape_13:0", shape=(?, 15), dtype=float32)
[2m[36m(pid=406195)[0m inputs: Tensor("default_policy/Reshape_13:0", shape=(?, 15), dtype=float32)
Result for PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a:
  custom_metrics: {}
  date: 2019-12-10_20-52-29
  done: false
  episode_len_mean: 6.237179487179487
  episode_reward_max: 2.0
  episode_reward_mean: -0.34294871794871795
  episode_reward_min: -1.0
  episodes_this_iter: 156
  episodes_total: 156
  experiment_id: 456eff59b97149079fb546278ee81886
  experiment_tag: '0'
  hostname: data-lounge-server-2
  info:
    grad_time_ms: 680.096
    learner:
      action_logp_max: 0.0
      action_logp_mean: -3.5734286308288574
      action_logp_min: -14.677698135375977
      latent_sample_part_0_max: 3.9099388122558594
      latent_sample_part_0_mean: -0.0003292834444437176
      latent_sample_part_0_min: -2.7940831184387207
      latent_sample_part_1_max: 3.9478952884674072
      latent_sample_part_1_mean: 0.012255735695362091
      latent_sample_part_1_min: -3.116400718688965
      latent_sample_part_2_max: 3.449237823486328
      latent_sample_part_2_mean: -0.00759899290278554
      latent_sample_part_2_min: -3.225440740585327
    num_steps_sampled: 1001
    num_steps_trained: 1001
    opt_peak_throughput: 1471.851
    opt_samples: 1001.0
    sample_peak_throughput: 204.423
    sample_time_ms: 4896.719
    update_time_ms: 765.29
  iterations_since_restore: 1
  node_ip: 10.64.129.23
  num_healthy_workers: 7
  off_policy_estimator: {}
  perf:
    gpu_util_percent0: 0.0
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    vram_util_percent0: 0.0
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 406245
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_env_wait_ms: 0.052502331061240956
    mean_inference_ms: 3.252853408583209
    mean_processing_ms: 0.31048730228021953
  tf_summary_inputs:
    histogram: {}
    scalar: {}
  time_since_restore: 6.347564458847046
  time_this_iter_s: 6.347564458847046
  time_total_s: 6.347564458847046
  timestamp: 1575971549
  timesteps_since_restore: 1001
  timesteps_this_iter: 1001
  timesteps_total: 1001
  training_iteration: 1
  trial_id: c050669a
  
== Status ==
Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)
Using FIFO scheduling algorithm.
Resources requested: 8/48 CPUs, 0/3 GPUs, 0.0/158.4 GiB heap, 0.0/12.84 GiB objects
Number of trials: 1 ({'RUNNING': 1})
Result logdir: /media/tb-2/user_data/jordan/projects/rl-copula-policy/data/gaussian_copula_v3_rnn_model_ReversedAddition3-v0-20191210T205217/gaussian_copula_v3_rnn_model_ReversedAddition3-v0
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+
| Trial name                                       | status   | loc                 |   iter |   total time (s) |   timesteps |    reward |
|--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------|
| PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a | RUNNING  | 10.64.129.23:406245 |      1 |          6.34756 |        1001 | -0.342949 |
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+

Result for PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a:
  custom_metrics: {}
  date: 2019-12-10_20-52-34
  done: false
  episode_len_mean: 7.27536231884058
  episode_reward_max: 1.5
  episode_reward_mean: -0.3115942028985507
  episode_reward_min: -1.0
  episodes_this_iter: 138
  episodes_total: 738
  experiment_id: 456eff59b97149079fb546278ee81886
  experiment_tag: '0'
  hostname: data-lounge-server-2
  info:
    grad_time_ms: 157.577
    learner:
      action_logp_max: 0.0
      action_logp_mean: -4.03821325302124
      action_logp_min: -17.054636001586914
      latent_sample_part_0_max: 3.21396541595459
      latent_sample_part_0_mean: 0.01867416873574257
      latent_sample_part_0_min: -4.061148643493652
      latent_sample_part_1_max: 3.2041070461273193
      latent_sample_part_1_mean: -0.005102640483528376
      latent_sample_part_1_min: -3.312206983566284
      latent_sample_part_2_max: 3.48457932472229
      latent_sample_part_2_mean: 0.007960214279592037
      latent_sample_part_2_min: -4.587353706359863
    num_steps_sampled: 5005
    num_steps_trained: 5005
    opt_peak_throughput: 6352.462
    opt_samples: 1001.0
    sample_peak_throughput: 797.311
    sample_time_ms: 1255.47
    update_time_ms: 155.072
  iterations_since_restore: 5
  node_ip: 10.64.129.23
  num_healthy_workers: 7
  off_policy_estimator: {}
  perf:
    gpu_util_percent0: 0.0
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    vram_util_percent0: 0.0
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 406245
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_env_wait_ms: 0.05242956494447258
    mean_inference_ms: 1.7670107272923905
    mean_processing_ms: 0.3097501952352239
  tf_summary_inputs:
    histogram: {}
    scalar: {}
  time_since_restore: 7.865440368652344
  time_this_iter_s: 0.30233192443847656
  time_total_s: 7.865440368652344
  timestamp: 1575971554
  timesteps_since_restore: 5005
  timesteps_this_iter: 1001
  timesteps_total: 5005
  training_iteration: 5
  trial_id: c050669a
  
== Status ==
Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)
Using FIFO scheduling algorithm.
Resources requested: 8/48 CPUs, 0/3 GPUs, 0.0/158.4 GiB heap, 0.0/12.84 GiB objects
Number of trials: 1 ({'RUNNING': 1})
Result logdir: /media/tb-2/user_data/jordan/projects/rl-copula-policy/data/gaussian_copula_v3_rnn_model_ReversedAddition3-v0-20191210T205217/gaussian_copula_v3_rnn_model_ReversedAddition3-v0
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+
| Trial name                                       | status   | loc                 |   iter |   total time (s) |   timesteps |    reward |
|--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------|
| PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a | RUNNING  | 10.64.129.23:406245 |      5 |          7.86544 |        5005 | -0.311594 |
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+

Result for PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a:
  custom_metrics: {}
  date: 2019-12-10_20-52-40
  done: false
  episode_len_mean: 6.644295302013423
  episode_reward_max: 2.0
  episode_reward_mean: -0.2785234899328859
  episode_reward_min: -1.0
  episodes_this_iter: 149
  episodes_total: 1623
  experiment_id: 456eff59b97149079fb546278ee81886
  experiment_tag: '0'
  hostname: data-lounge-server-2
  info:
    grad_time_ms: 26.723
    learner:
      action_logp_max: 0.0
      action_logp_mean: -3.753429412841797
      action_logp_min: -14.023221969604492
      latent_sample_part_0_max: 3.098947286605835
      latent_sample_part_0_mean: 0.013064941391348839
      latent_sample_part_0_min: -3.080852508544922
      latent_sample_part_1_max: 2.896226406097412
      latent_sample_part_1_mean: -0.01291806623339653
      latent_sample_part_1_min: -2.854491710662842
      latent_sample_part_2_max: 3.677659273147583
      latent_sample_part_2_mean: 0.010878722183406353
      latent_sample_part_2_min: -3.445594549179077
    num_steps_sampled: 11011
    num_steps_trained: 11011
    opt_peak_throughput: 37457.83
    opt_samples: 1001.0
    sample_peak_throughput: 3242.713
    sample_time_ms: 308.692
    update_time_ms: 2.385
  iterations_since_restore: 11
  node_ip: 10.64.129.23
  num_healthy_workers: 7
  off_policy_estimator: {}
  perf:
    gpu_util_percent0: 0.0
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    vram_util_percent0: 0.0
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 406245
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_env_wait_ms: 0.053155978594584034
    mean_inference_ms: 1.4823922148203281
    mean_processing_ms: 0.3154881233543311
  tf_summary_inputs:
    histogram: {}
    scalar: {}
  time_since_restore: 9.774438858032227
  time_this_iter_s: 0.3213052749633789
  time_total_s: 9.774438858032227
  timestamp: 1575971560
  timesteps_since_restore: 11011
  timesteps_this_iter: 1001
  timesteps_total: 11011
  training_iteration: 11
  trial_id: c050669a
  
== Status ==
Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)
Using FIFO scheduling algorithm.
Resources requested: 8/48 CPUs, 0/3 GPUs, 0.0/158.4 GiB heap, 0.0/12.84 GiB objects
Number of trials: 1 ({'RUNNING': 1})
Result logdir: /media/tb-2/user_data/jordan/projects/rl-copula-policy/data/gaussian_copula_v3_rnn_model_ReversedAddition3-v0-20191210T205217/gaussian_copula_v3_rnn_model_ReversedAddition3-v0
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+
| Trial name                                       | status   | loc                 |   iter |   total time (s) |   timesteps |    reward |
|--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------|
| PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a | RUNNING  | 10.64.129.23:406245 |     11 |          9.77444 |       11011 | -0.278523 |
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+

Result for PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a:
  custom_metrics: {}
  date: 2019-12-10_20-52-46
  done: false
  episode_len_mean: 6.620915032679738
  episode_reward_max: 1.5
  episode_reward_mean: -0.4215686274509804
  episode_reward_min: -1.0
  episodes_this_iter: 153
  episodes_total: 2518
  experiment_id: 456eff59b97149079fb546278ee81886
  experiment_tag: '0'
  hostname: data-lounge-server-2
  info:
    grad_time_ms: 26.929
    learner:
      action_logp_max: 0.0
      action_logp_mean: -3.7703611850738525
      action_logp_min: -16.515277862548828
      latent_sample_part_0_max: 3.1169090270996094
      latent_sample_part_0_mean: -0.011266963556408882
      latent_sample_part_0_min: -2.954418897628784
      latent_sample_part_1_max: 3.512772560119629
      latent_sample_part_1_mean: -0.017063044011592865
      latent_sample_part_1_min: -3.778190851211548
      latent_sample_part_2_max: 2.9514243602752686
      latent_sample_part_2_mean: 0.016723226755857468
      latent_sample_part_2_min: -3.341646909713745
    num_steps_sampled: 17017
    num_steps_trained: 17017
    opt_peak_throughput: 37171.497
    opt_samples: 1001.0
    sample_peak_throughput: 3521.057
    sample_time_ms: 284.29
    update_time_ms: 2.355
  iterations_since_restore: 17
  node_ip: 10.64.129.23
  num_healthy_workers: 7
  off_policy_estimator: {}
  perf:
    gpu_util_percent0: 0.0
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    vram_util_percent0: 0.0
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 406245
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_env_wait_ms: 0.05343038041566219
    mean_inference_ms: 1.4036426901827572
    mean_processing_ms: 0.31813811658172625
  tf_summary_inputs:
    histogram: {}
    scalar: {}
  time_since_restore: 11.70100736618042
  time_this_iter_s: 0.3213770389556885
  time_total_s: 11.70100736618042
  timestamp: 1575971566
  timesteps_since_restore: 17017
  timesteps_this_iter: 1001
  timesteps_total: 17017
  training_iteration: 17
  trial_id: c050669a
  
== Status ==
Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)
Using FIFO scheduling algorithm.
Resources requested: 8/48 CPUs, 0/3 GPUs, 0.0/158.4 GiB heap, 0.0/12.84 GiB objects
Number of trials: 1 ({'RUNNING': 1})
Result logdir: /media/tb-2/user_data/jordan/projects/rl-copula-policy/data/gaussian_copula_v3_rnn_model_ReversedAddition3-v0-20191210T205217/gaussian_copula_v3_rnn_model_ReversedAddition3-v0
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+
| Trial name                                       | status   | loc                 |   iter |   total time (s) |   timesteps |    reward |
|--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------|
| PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a | RUNNING  | 10.64.129.23:406245 |     17 |           11.701 |       17017 | -0.421569 |
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+

Result for PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a:
  custom_metrics: {}
  date: 2019-12-10_20-52-52
  done: false
  episode_len_mean: 6.371069182389937
  episode_reward_max: 2.0
  episode_reward_mean: -0.2138364779874214
  episode_reward_min: -1.0
  episodes_this_iter: 159
  episodes_total: 3435
  experiment_id: 456eff59b97149079fb546278ee81886
  experiment_tag: '0'
  hostname: data-lounge-server-2
  info:
    grad_time_ms: 27.086
    learner:
      action_logp_max: 0.0
      action_logp_mean: -3.657701015472412
      action_logp_min: -16.01424217224121
      latent_sample_part_0_max: 4.009291172027588
      latent_sample_part_0_mean: -0.03841925784945488
      latent_sample_part_0_min: -3.3874731063842773
      latent_sample_part_1_max: 3.062653064727783
      latent_sample_part_1_mean: 0.04202423244714737
      latent_sample_part_1_min: -3.845153331756592
      latent_sample_part_2_max: 2.9084789752960205
      latent_sample_part_2_mean: -0.011831208132207394
      latent_sample_part_2_min: -3.209536075592041
    num_steps_sampled: 23023
    num_steps_trained: 23023
    opt_peak_throughput: 36956.855
    opt_samples: 1001.0
    sample_peak_throughput: 3521.458
    sample_time_ms: 284.257
    update_time_ms: 2.138
  iterations_since_restore: 23
  node_ip: 10.64.129.23
  num_healthy_workers: 7
  off_policy_estimator: {}
  perf:
    gpu_util_percent0: 0.0
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    vram_util_percent0: 0.0
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 406245
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_env_wait_ms: 0.05345022313074466
    mean_inference_ms: 1.3581284781906646
    mean_processing_ms: 0.31872253613403534
  tf_summary_inputs:
    histogram: {}
    scalar: {}
  time_since_restore: 13.573235511779785
  time_this_iter_s: 0.3152623176574707
  time_total_s: 13.573235511779785
  timestamp: 1575971572
  timesteps_since_restore: 23023
  timesteps_this_iter: 1001
  timesteps_total: 23023
  training_iteration: 23
  trial_id: c050669a
  
== Status ==
Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)
Using FIFO scheduling algorithm.
Resources requested: 8/48 CPUs, 0/3 GPUs, 0.0/158.4 GiB heap, 0.0/12.84 GiB objects
Number of trials: 1 ({'RUNNING': 1})
Result logdir: /media/tb-2/user_data/jordan/projects/rl-copula-policy/data/gaussian_copula_v3_rnn_model_ReversedAddition3-v0-20191210T205217/gaussian_copula_v3_rnn_model_ReversedAddition3-v0
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+
| Trial name                                       | status   | loc                 |   iter |   total time (s) |   timesteps |    reward |
|--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------|
| PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a | RUNNING  | 10.64.129.23:406245 |     23 |          13.5732 |       23023 | -0.213836 |
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+

Result for PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a:
  custom_metrics: {}
  date: 2019-12-10_20-52-58
  done: false
  episode_len_mean: 6.4423076923076925
  episode_reward_max: 1.5
  episode_reward_mean: -0.40384615384615385
  episode_reward_min: -1.0
  episodes_this_iter: 156
  episodes_total: 4354
  experiment_id: 456eff59b97149079fb546278ee81886
  experiment_tag: '0'
  hostname: data-lounge-server-2
  info:
    grad_time_ms: 26.872
    learner:
      action_logp_max: 0.0
      action_logp_mean: -3.7173900604248047
      action_logp_min: -15.217643737792969
      latent_sample_part_0_max: 2.9209680557250977
      latent_sample_part_0_mean: -0.01306922733783722
      latent_sample_part_0_min: -3.1360392570495605
      latent_sample_part_1_max: 3.5942251682281494
      latent_sample_part_1_mean: -0.01884349249303341
      latent_sample_part_1_min: -3.3765876293182373
      latent_sample_part_2_max: 3.3187646865844727
      latent_sample_part_2_mean: -0.01726214773952961
      latent_sample_part_2_min: -2.910004138946533
    num_steps_sampled: 29029
    num_steps_trained: 29029
    opt_peak_throughput: 37249.988
    opt_samples: 1001.0
    sample_peak_throughput: 3507.203
    sample_time_ms: 285.413
    update_time_ms: 2.231
  iterations_since_restore: 29
  node_ip: 10.64.129.23
  num_healthy_workers: 7
  off_policy_estimator: {}
  perf:
    gpu_util_percent0: 0.0
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    vram_util_percent0: 0.0
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 406245
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_env_wait_ms: 0.05366733396772233
    mean_inference_ms: 1.338794170886081
    mean_processing_ms: 0.32059413834135536
  tf_summary_inputs:
    histogram: {}
    scalar: {}
  time_since_restore: 15.496289014816284
  time_this_iter_s: 0.34102416038513184
  time_total_s: 15.496289014816284
  timestamp: 1575971578
  timesteps_since_restore: 29029
  timesteps_this_iter: 1001
  timesteps_total: 29029
  training_iteration: 29
  trial_id: c050669a
  
== Status ==
Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)
Using FIFO scheduling algorithm.
Resources requested: 8/48 CPUs, 0/3 GPUs, 0.0/158.4 GiB heap, 0.0/12.84 GiB objects
Number of trials: 1 ({'RUNNING': 1})
Result logdir: /media/tb-2/user_data/jordan/projects/rl-copula-policy/data/gaussian_copula_v3_rnn_model_ReversedAddition3-v0-20191210T205217/gaussian_copula_v3_rnn_model_ReversedAddition3-v0
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+
| Trial name                                       | status   | loc                 |   iter |   total time (s) |   timesteps |    reward |
|--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------|
| PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a | RUNNING  | 10.64.129.23:406245 |     29 |          15.4963 |       29029 | -0.403846 |
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+

Result for PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a:
  custom_metrics: {}
  date: 2019-12-10_20-53-04
  done: false
  episode_len_mean: 6.409090909090909
  episode_reward_max: 3.0
  episode_reward_mean: -0.32792207792207795
  episode_reward_min: -1.0
  episodes_this_iter: 154
  episodes_total: 5256
  experiment_id: 456eff59b97149079fb546278ee81886
  experiment_tag: '0'
  hostname: data-lounge-server-2
  info:
    grad_time_ms: 26.951
    learner:
      action_logp_max: 0.0
      action_logp_mean: -3.765411615371704
      action_logp_min: -13.834610939025879
      latent_sample_part_0_max: 3.3559718132019043
      latent_sample_part_0_mean: -0.013475315645337105
      latent_sample_part_0_min: -3.4442739486694336
      latent_sample_part_1_max: 3.642468214035034
      latent_sample_part_1_mean: -0.0031576917972415686
      latent_sample_part_1_min: -3.061087131500244
      latent_sample_part_2_max: 3.0693771839141846
      latent_sample_part_2_mean: -0.014111530035734177
      latent_sample_part_2_min: -2.950942039489746
    num_steps_sampled: 35035
    num_steps_trained: 35035
    opt_peak_throughput: 37140.949
    opt_samples: 1001.0
    sample_peak_throughput: 3557.17
    sample_time_ms: 281.403
    update_time_ms: 2.198
  iterations_since_restore: 35
  node_ip: 10.64.129.23
  num_healthy_workers: 7
  off_policy_estimator: {}
  perf:
    gpu_util_percent0: 0.0
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    vram_util_percent0: 0.0
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 406245
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_env_wait_ms: 0.05344487828165439
    mean_inference_ms: 1.3197883904438275
    mean_processing_ms: 0.3196598472433705
  tf_summary_inputs:
    histogram: {}
    scalar: {}
  time_since_restore: 17.331204891204834
  time_this_iter_s: 0.30031466484069824
  time_total_s: 17.331204891204834
  timestamp: 1575971584
  timesteps_since_restore: 35035
  timesteps_this_iter: 1001
  timesteps_total: 35035
  training_iteration: 35
  trial_id: c050669a
  
== Status ==
Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)
Using FIFO scheduling algorithm.
Resources requested: 8/48 CPUs, 0/3 GPUs, 0.0/158.4 GiB heap, 0.0/12.84 GiB objects
Number of trials: 1 ({'RUNNING': 1})
Result logdir: /media/tb-2/user_data/jordan/projects/rl-copula-policy/data/gaussian_copula_v3_rnn_model_ReversedAddition3-v0-20191210T205217/gaussian_copula_v3_rnn_model_ReversedAddition3-v0
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+
| Trial name                                       | status   | loc                 |   iter |   total time (s) |   timesteps |    reward |
|--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------|
| PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a | RUNNING  | 10.64.129.23:406245 |     35 |          17.3312 |       35035 | -0.327922 |
+--------------------------------------------------+----------+---------------------+--------+------------------+-------------+-----------+

[2m[36m(pid=406245)[0m 2019-12-10 20:53:06,290	ERROR tf_run_builder.py:51 -- Error fetching: [<tf.Operation 'default_policy/Adam' type=AssignAdd>, {'learner_stats': {'action_logp_min': <tf.Tensor 'default_policy/Min:0' shape=() dtype=float32>, 'action_logp_max': <tf.Tensor 'default_policy/Max_2:0' shape=() dtype=float32>, 'action_logp_mean': <tf.Tensor 'default_policy/Mean_5:0' shape=() dtype=float32>, 'latent_sample_part_0_min': <tf.Tensor 'default_policy/Min_1:0' shape=() dtype=float32>, 'latent_sample_part_0_mean': <tf.Tensor 'default_policy/Mean_6:0' shape=() dtype=float32>, 'latent_sample_part_0_max': <tf.Tensor 'default_policy/Max_3:0' shape=() dtype=float32>, 'latent_sample_part_1_min': <tf.Tensor 'default_policy/Min_2:0' shape=() dtype=float32>, 'latent_sample_part_1_mean': <tf.Tensor 'default_policy/Mean_7:0' shape=() dtype=float32>, 'latent_sample_part_1_max': <tf.Tensor 'default_policy/Max_4:0' shape=() dtype=float32>, 'latent_sample_part_2_min': <tf.Tensor 'default_policy/Min_3:0' shape=() dtype=float32>, 'latent_sample_part_2_mean': <tf.Tensor 'default_policy/Mean_8:0' shape=() dtype=float32>, 'latent_sample_part_2_max': <tf.Tensor 'default_policy/Max_5:0' shape=() dtype=float32>, 'model': {}}}], feed_dict={<tf.Tensor 'default_policy/action:0' shape=(?, 9) dtype=float32>: array([[0.2228871 , 0.09453787, 0.69448477, ..., 0.        , 0.        ,
[2m[36m(pid=406245)[0m         0.        ],
[2m[36m(pid=406245)[0m        [0.6607151 , 0.2774151 , 0.93066472, ..., 1.        , 0.        ,
[2m[36m(pid=406245)[0m         2.        ],
[2m[36m(pid=406245)[0m        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
[2m[36m(pid=406245)[0m         0.        ],
[2m[36m(pid=406245)[0m        ...,
[2m[36m(pid=406245)[0m        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
[2m[36m(pid=406245)[0m         0.        ],
[2m[36m(pid=406245)[0m        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
[2m[36m(pid=406245)[0m         0.        ],
[2m[36m(pid=406245)[0m        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
[2m[36m(pid=406245)[0m         0.        ]]), <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>: array([0., 0., 0., ..., 0., 0., 0.]), <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>: array([[0., 0., 0., 1.],
[2m[36m(pid=406245)[0m        [0., 0., 0., 1.],
[2m[36m(pid=406245)[0m        [0., 0., 0., 0.],
[2m[36m(pid=406245)[0m        ...,
[2m[36m(pid=406245)[0m        [0., 0., 0., 0.],
[2m[36m(pid=406245)[0m        [0., 0., 0., 0.],
[2m[36m(pid=406245)[0m        [0., 0., 0., 0.]]), <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>: array([-7.75231266, -8.44525528,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m         0.        ,  0.        ]), <tf.Tensor 'default_policy/actions:0' shape=(?, 9) dtype=float32>: array([[0.6607151 , 0.2774151 , 0.93066472, ..., 1.        , 0.        ,
[2m[36m(pid=406245)[0m         2.        ],
[2m[36m(pid=406245)[0m        [0.17664698, 0.96531814, 0.06079421, ..., 0.        , 1.        ,
[2m[36m(pid=406245)[0m         0.        ],
[2m[36m(pid=406245)[0m        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
[2m[36m(pid=406245)[0m         0.        ],
[2m[36m(pid=406245)[0m        ...,
[2m[36m(pid=406245)[0m        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
[2m[36m(pid=406245)[0m         0.        ],
[2m[36m(pid=406245)[0m        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
[2m[36m(pid=406245)[0m         0.        ],
[2m[36m(pid=406245)[0m        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
[2m[36m(pid=406245)[0m         0.        ]]), <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>: array([0.27392203, 0.29968655, 0.        , ..., 0.        , 0.        ,
[2m[36m(pid=406245)[0m        0.        ]), <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>: array([-0.495, -0.5  ,  0.   , ...,  0.   ,  0.   ,  0.   ]), <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 64) dtype=float32>: array([[-0.08458976, -0.06900841,  0.06588159, ..., -0.15012486,
[2m[36m(pid=406245)[0m          0.04272404,  0.01120198],
[2m[36m(pid=406245)[0m        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m          0.        ,  0.        ],
[2m[36m(pid=406245)[0m        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m          0.        ,  0.        ],
[2m[36m(pid=406245)[0m        ...,
[2m[36m(pid=406245)[0m        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m          0.        ,  0.        ],
[2m[36m(pid=406245)[0m        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m          0.        ,  0.        ],
[2m[36m(pid=406245)[0m        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m          0.        ,  0.        ]], dtype=float32), <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 64) dtype=float32>: array([[-0.16117339, -0.14712006,  0.13391808, ..., -0.29060155,
[2m[36m(pid=406245)[0m          0.08496217,  0.02327602],
[2m[36m(pid=406245)[0m        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m          0.        ,  0.        ],
[2m[36m(pid=406245)[0m        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m          0.        ,  0.        ],
[2m[36m(pid=406245)[0m        ...,
[2m[36m(pid=406245)[0m        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m          0.        ,  0.        ],
[2m[36m(pid=406245)[0m        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m          0.        ,  0.        ],
[2m[36m(pid=406245)[0m        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
[2m[36m(pid=406245)[0m          0.        ,  0.        ]], dtype=float32), <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>: array([ 2,  4, 13,  6,  1,  2,  6,  7, 11,  9,  1,  9,  6,  8, 11,  5, 11,
[2m[36m(pid=406245)[0m        11,  1,  2,  9,  1,  7,  1,  9,  1, 11,  3,  5,  9,  4,  6, 11,  9,
[2m[36m(pid=406245)[0m         6,  1, 11, 11, 11,  2,  2,  1, 11,  6,  4,  2,  3,  3, 12,  6, 10,
[2m[36m(pid=406245)[0m         1,  1,  9,  9, 13,  5,  9,  2,  1,  2,  7,  8,  3, 11,  2, 13,  1,
[2m[36m(pid=406245)[0m        11,  3,  4,  3,  6,  6,  9,  8,  9,  9,  3,  2,  3,  3,  7, 11, 13,
[2m[36m(pid=406245)[0m         5, 10, 12,  1,  9,  6,  8,  7,  6,  7,  3,  2,  3,  4,  2, 13,  5,
[2m[36m(pid=406245)[0m        12,  4,  4,  9,  9,  3,  8,  9,  2,  1,  1,  2, 10,  1,  4,  3,  9,
[2m[36m(pid=406245)[0m         4,  6,  6,  9, 11,  9,  9, 11, 13,  2,  4,  9,  8,  2,  5,  3,  7,
[2m[36m(pid=406245)[0m        13, 12,  1,  2,  8, 11,  8,  9,  9,  9,  2,  2,  7, 11, 11,  7, 12,
[2m[36m(pid=406245)[0m         8,  8,  1, 11,  6]), <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>: True}
[2m[36m(pid=406245)[0m Traceback (most recent call last):
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1365, in _do_call
[2m[36m(pid=406245)[0m     return fn(*args)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1350, in _run_fn
[2m[36m(pid=406245)[0m     target_list, run_metadata)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1443, in _call_tf_sessionrun
[2m[36m(pid=406245)[0m     run_metadata)
[2m[36m(pid=406245)[0m tensorflow.python.framework.errors_impl.InvalidArgumentError: Cholesky decomposition was not successful. The input might not be valid.
[2m[36m(pid=406245)[0m 	 [[{{node default_policy/Cholesky_1}}]]
[2m[36m(pid=406245)[0m 
[2m[36m(pid=406245)[0m During handling of the above exception, another exception occurred:
[2m[36m(pid=406245)[0m 
[2m[36m(pid=406245)[0m Traceback (most recent call last):
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/utils/tf_run_builder.py", line 48, in get
[2m[36m(pid=406245)[0m     self.feed_dict, os.environ.get("TF_TIMELINE_DIR"))
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/utils/tf_run_builder.py", line 94, in run_timeline
[2m[36m(pid=406245)[0m     fetches = sess.run(ops, feed_dict=feed_dict)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 956, in run
[2m[36m(pid=406245)[0m     run_metadata_ptr)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1180, in _run
[2m[36m(pid=406245)[0m     feed_dict_tensor, options, run_metadata)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
[2m[36m(pid=406245)[0m     run_metadata)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
[2m[36m(pid=406245)[0m     raise type(e)(node_def, op, message)
[2m[36m(pid=406245)[0m tensorflow.python.framework.errors_impl.InvalidArgumentError: Cholesky decomposition was not successful. The input might not be valid.
[2m[36m(pid=406245)[0m 	 [[node default_policy/Cholesky_1 (defined at /media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
[2m[36m(pid=406245)[0m 
[2m[36m(pid=406245)[0m Original stack trace for 'default_policy/Cholesky_1':
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/workers/default_worker.py", line 99, in <module>
[2m[36m(pid=406245)[0m     ray.worker.global_worker.main_loop()
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/worker.py", line 433, in main_loop
[2m[36m(pid=406245)[0m     self.core_worker.run_task_loop()
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/function_manager.py", line 766, in actor_method_executor
[2m[36m(pid=406245)[0m     method_returns = method(actor, *args, **kwargs)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 90, in __init__
[2m[36m(pid=406245)[0m     Trainer.__init__(self, config, env, logger_creator)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 398, in __init__
[2m[36m(pid=406245)[0m     Trainable.__init__(self, config, logger_creator)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/tune/trainable.py", line 96, in __init__
[2m[36m(pid=406245)[0m     self._setup(copy.deepcopy(self.config))
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 523, in _setup
[2m[36m(pid=406245)[0m     self._init(self.config, self.env_creator)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 109, in _init
[2m[36m(pid=406245)[0m     self.config["num_workers"])
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 568, in _make_workers
[2m[36m(pid=406245)[0m     logdir=self.logdir)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 64, in __init__
[2m[36m(pid=406245)[0m     RolloutWorker, env_creator, policy, 0, self._local_config)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 220, in _make_worker
[2m[36m(pid=406245)[0m     _fake_sampler=config.get("_fake_sampler", False))
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 356, in __init__
[2m[36m(pid=406245)[0m     self._build_policy_map(policy_dict, policy_config)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 772, in _build_policy_map
[2m[36m(pid=406245)[0m     policy_map[name] = cls(obs_space, act_space, merged_conf)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy_template.py", line 143, in __init__
[2m[36m(pid=406245)[0m     obs_include_prev_action_reward=obs_include_prev_action_reward)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 200, in __init__
[2m[36m(pid=406245)[0m     self._initialize_loss()
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 341, in _initialize_loss
[2m[36m(pid=406245)[0m     loss = self._do_loss_init(train_batch)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 353, in _do_loss_init
[2m[36m(pid=406245)[0m     loss = self._loss_fn(self, self.model, self.dist_class, train_batch)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/rl_copula_policy/policies/pg_copula_policy.py", line 9, in compute_loss
[2m[36m(pid=406245)[0m     loss = pg_loss(policy, model, dist_class, train_batch)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/rl_copula_policy/policies/pg_policy.py", line 45, in policy_gradient_loss
[2m[36m(pid=406245)[0m     action_dist = dist_class(flat_action_params, model)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/rl_copula_policy/action_distributions/gaussian_copula_action_distribution.py", line 62, in __init__
[2m[36m(pid=406245)[0m     num_marginals=self.num_marginals)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/rl_copula_policy/action_distributions/gaussian_copula_action_distribution.py", line 149, in _init_latent_dist
[2m[36m(pid=406245)[0m     self._latent_dist = GaussianCopula(latent_means, self._latent_covariance_tril)
[2m[36m(pid=406245)[0m   File "</media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/decorator.py:decorator-gen-244>", line 2, in __init__
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_probability/python/distributions/distribution.py", line 276, in wrapped_init
[2m[36m(pid=406245)[0m     default_init(self_, *args, **kwargs)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/rl_copula_policy/tf_distributions/gaussian_copula.py", line 15, in __init__
[2m[36m(pid=406245)[0m     scale_tril = tf.linalg.set_diag(tf.linalg.cholesky(scale_mat), diagonal)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_linalg_ops.py", line 820, in cholesky
[2m[36m(pid=406245)[0m     "Cholesky", input=input, name=name)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
[2m[36m(pid=406245)[0m     op_def=op_def)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
[2m[36m(pid=406245)[0m     return func(*args, **kwargs)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3360, in create_op
[2m[36m(pid=406245)[0m     attrs, op_def, compute_device)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3429, in _create_op_internal
[2m[36m(pid=406245)[0m     op_def=op_def)
[2m[36m(pid=406245)[0m   File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1751, in __init__2019-12-10 20:53:06,800	ERROR trial_runner.py:476 -- Error processing event.
Traceback (most recent call last):
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 420, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 378, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/worker.py", line 1450, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ValueError): [36mray_worker[39m (pid=406245, ip=10.64.129.23)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1350, in _run_fn
    target_list, run_metadata)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cholesky decomposition was not successful. The input might not be valid.
	 [[{{node default_policy/Cholesky_1}}]]

During handling of the above exception, another exception occurred:

[36mray_worker[39m (pid=406245, ip=10.64.129.23)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/utils/tf_run_builder.py", line 48, in get
    self.feed_dict, os.environ.get("TF_TIMELINE_DIR"))
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/utils/tf_run_builder.py", line 94, in run_timeline
    fetches = sess.run(ops, feed_dict=feed_dict)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cholesky decomposition was not successful. The input might not be valid.
	 [[node default_policy/Cholesky_1 (defined at /media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]

Original stack trace for 'default_policy/Cholesky_1':
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/workers/default_worker.py", line 99, in <module>
    ray.worker.global_worker.main_loop()
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 90, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 398, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/tune/trainable.py", line 96, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 523, in _setup
    self._init(self.config, self.env_creator)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 109, in _init
    self.config["num_workers"])
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 568, in _make_workers
    logdir=self.logdir)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 64, in __init__
    RolloutWorker, env_creator, policy, 0, self._local_config)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 220, in _make_worker
    _fake_sampler=config.get("_fake_sampler", False))
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 356, in __init__
    self._build_policy_map(policy_dict, policy_config)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 772, in _build_policy_map
    policy_map[name] = cls(obs_space, act_space, merged_conf)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy_template.py", line 143, in __init__
    obs_include_prev_action_reward=obs_include_prev_action_reward)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 200, in __init__
    self._initialize_loss()
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 341, in _initialize_loss
    loss = self._do_loss_init(train_batch)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 353, in _do_loss_init
    loss = self._loss_fn(self, self.model, self.dist_class, train_batch)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/rl_copula_policy/policies/pg_copula_policy.py", line 9, in compute_loss
    loss = pg_loss(policy, model, dist_class, train_batch)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/rl_copula_policy/policies/pg_policy.py", line 45, in policy_gradient_loss
    action_dist = dist_class(flat_action_params, model)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/rl_copula_policy/action_distributions/gaussian_copula_action_distribution.py", line 62, in __init__
    num_marginals=self.num_marginals)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/rl_copula_policy/action_distributions/gaussian_copula_action_distribution.py", line 149, in _init_latent_dist
    self._latent_dist = GaussianCopula(latent_means, self._latent_covariance_tril)
  File "</media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/decorator.py:decorator-gen-244>", line 2, in __init__
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_probability/python/distributions/distribution.py", line 276, in wrapped_init
    default_init(self_, *args, **kwargs)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/rl_copula_policy/tf_distributions/gaussian_copula.py", line 15, in __init__
    scale_tril = tf.linalg.set_diag(tf.linalg.cholesky(scale_mat), diagonal)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_linalg_ops.py", line 820, in cholesky
    "Cholesky", input=input, name=name)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
    op_def=op_def)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3360, in create_op
    attrs, op_def, compute_device)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3429, in _create_op_internal
    op_def=op_def)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1751, in __init__
    self._traceback = tf_stack.extract_stack()


During handling of the above exception, another exception occurred:

[36mray_worker[39m (pid=406245, ip=10.64.129.23)
  File "python/ray/_raylet.pyx", line 600, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 601, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 603, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 586, in ray._raylet.execute_task.function_executor
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 447, in train
    raise e
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 433, in train
    result = Trainable.train(self)
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/tune/trainable.py", line 176, in train
    result = self._train()
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 129, in _train
    fetches = self.optimizer.step()
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/optimizers/sync_samples_optimizer.py", line 100, in step
    }, minibatch.count)))[policy_id]
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 616, in learn_on_batch
    info_out.update({k: builder.get(v) for k, v in to_fetch.items()})
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 616, in <dictcomp>
    info_out.update({k: builder.get(v) for k, v in to_fetch.items()})
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/rllib/utils/tf_run_builder.py", line 53, in get
    self.fetches, self.feed_dict))
ValueError: Error fetching: [<tf.Operation 'default_policy/Adam' type=AssignAdd>, {'learner_stats': {'action_logp_min': <tf.Tensor 'default_policy/Min:0' shape=() dtype=float32>, 'action_logp_max': <tf.Tensor 'default_policy/Max_2:0' shape=() dtype=float32>, 'action_logp_mean': <tf.Tensor 'default_policy/Mean_5:0' shape=() dtype=float32>, 'latent_sample_part_0_min': <tf.Tensor 'default_policy/Min_1:0' shape=() dtype=float32>, 'latent_sample_part_0_mean': <tf.Tensor 'default_policy/Mean_6:0' shape=() dtype=float32>, 'latent_sample_part_0_max': <tf.Tensor 'default_policy/Max_3:0' shape=() dtype=float32>, 'latent_sample_part_1_min': <tf.Tensor 'default_policy/Min_2:0' shape=() dtype=float32>, 'latent_sample_part_1_mean': <tf.Tensor 'default_policy/Mean_7:0' shape=() dtype=float32>, 'latent_sample_part_1_max': <tf.Tensor 'default_policy/Max_4:0' shape=() dtype=float32>, 'latent_sample_part_2_min': <tf.Tensor 'default_policy/Min_3:0' shape=() dtype=float32>, 'latent_sample_part_2_mean': <tf.Tensor 'default_policy/Mean_8:0' shape=() dtype=float32>, 'latent_sample_part_2_max': <tf.Tensor 'default_policy/Max_5:0' shape=() dtype=float32>, 'model': {}}}], feed_dict={<tf.Tensor 'default_policy/action:0' shape=(?, 9) dtype=float32>: array([[0.2228871 , 0.09453787, 0.69448477, ..., 0.        , 0.        ,
        0.        ],
       [0.6607151 , 0.2774151 , 0.93066472, ..., 1.        , 0.        ,
        2.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       ...,
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ]]), <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>: array([0., 0., 0., ..., 0., 0., 0.]), <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>: array([[0., 0., 0., 1.],
       [0., 0., 0., 1.],
       [0., 0., 0., 0.],
       ...,
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.]]), <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>: array([-7.75231266, -8.44525528,  0.        , ...,  0.        ,
        0.        ,  0.        ]), <tf.Tensor 'default_policy/actions:0' shape=(?, 9) dtype=float32>: array([[0.6607151 , 0.2774151 , 0.93066472, ..., 1.        , 0.        ,
        2.        ],
       [0.17664698, 0.96531814, 0.06079421, ..., 0.        , 1.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       ...,
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ]]), <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>: array([0.27392203, 0.29968655, 0.        , ..., 0.        , 0.        ,
       0.        ]), <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>: array([-0.495, -0.5  ,  0.   , ...,  0.   ,  0.   ,  0.   ]), <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 64) dtype=float32>: array([[-0.08458976, -0.06900841,  0.06588159, ..., -0.15012486,
         0.04272404,  0.01120198],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       ...,
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ]], dtype=float32), <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 64) dtype=float32>: array([[-0.16117339, -0.14712006,  0.13391808, ..., -0.29060155,
         0.08496217,  0.02327602],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       ...,
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.        ,  0.        ]], dtype=float32), <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>: array([ 2,  4, 13,  6,  1,  2,  6,  7, 11,  9,  1,  9,  6,  8, 11,  5, 11,
       11,  1,  2,  9,  1,  7,  1,  9,  1, 11,  3,  5,  9,  4,  6, 11,  9,
        6,  1, 11, 11, 11,  2,  2,  1, 11,  6,  4,  2,  3,  3, 12,  6, 10,
        1,  1,  9,  9, 13,  5,  9,  2,  1,  2,  7,  8,  3, 11,  2, 13,  1,
       11,  3,  4,  3,  6,  6,  9,  8,  9,  9,  3,  2,  3,  3,  7, 11, 13,
        5, 10, 12,  1,  9,  6,  8,  7,  6,  7,  3,  2,  3,  4,  2, 13,  5,
       12,  4,  4,  9,  9,  3,  8,  9,  2,  1,  1,  2, 10,  1,  4,  3,  9,
        4,  6,  6,  9, 11,  9,  9, 11, 13,  2,  4,  9,  8,  2,  5,  3,  7,
       13, 12,  1,  2,  8, 11,  8,  9,  9,  9,  2,  2,  7, 11, 11,  7, 12,
        8,  8,  1, 11,  6]), <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>: True}

[2m[36m(pid=406245)[0m     self._traceback = tf_stack.extract_stack()
[2m[36m(pid=406245)[0m 
== Status ==
Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)
Using FIFO scheduling algorithm.
Resources requested: 0/48 CPUs, 0/3 GPUs, 0.0/158.4 GiB heap, 0.0/12.84 GiB objects
Number of trials: 1 ({'ERROR': 1})
Result logdir: /media/tb-2/user_data/jordan/projects/rl-copula-policy/data/gaussian_copula_v3_rnn_model_ReversedAddition3-v0-20191210T205217/gaussian_copula_v3_rnn_model_ReversedAddition3-v0
+--------------------------------------------------+----------+-------+--------+------------------+-------------+-----------+
| Trial name                                       | status   | loc   |   iter |   total time (s) |   timesteps |    reward |
|--------------------------------------------------+----------+-------+--------+------------------+-------------+-----------|
| PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a | ERROR    |       |     35 |          17.3312 |       35035 | -0.327922 |
+--------------------------------------------------+----------+-------+--------+------------------+-------------+-----------+
Number of errored trials: 1
+--------------------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name                                       |   # failures | error file                                                                                                                                                                                                                                                             |
|--------------------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a |            1 | /media/tb-2/user_data/jordan/projects/rl-copula-policy/data/gaussian_copula_v3_rnn_model_ReversedAddition3-v0-20191210T205217/gaussian_copula_v3_rnn_model_ReversedAddition3-v0/PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a_2019-12-10_20-52-17hyiwqzd2/error.txt |
+--------------------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Traceback (most recent call last):
  File "./rl_copula_policy/experiments/discrete_action_agent.py", line 55, in <module>
    'use_vf_adv': True
  File "/media/tb-2/user_data/jordan/projects/rl-copula-policy/venv/lib/python3.6/site-packages/ray/tune/tune.py", line 327, in run
    raise TuneError("Trials did not complete", errored_trials)
ray.tune.error.TuneError: ('Trials did not complete', [PGCopulaTrainer_GausCopulaGymEnvWrapper_c050669a])
[2m[36m(pid=406228)[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.
[2m[36m(pid=406230)[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.
[2m[36m(pid=406202)[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.
[2m[36m(pid=406233)[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.
[2m[36m(pid=406216)[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.
